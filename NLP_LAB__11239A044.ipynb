{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karanambharath9182/11239A044-/blob/main/NLP_LAB__11239A044.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION"
      ],
      "metadata": {
        "id": "ZIiWyPOwt9T9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CGyzI5AQGxI",
        "outputId": "bb4492ad-7f9c-4937-9b59-74659d6b349b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Bharath bunny satvik teja.']\n",
            "Words: ['Bharath', 'bunny', 'satvik', 'teja', '.']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Program: Sentence and Word Tokenization using NLTK\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Input text\n",
        "text = \"Bharath bunny satvik teja.\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "\n",
        "# Word tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)\n",
        "\n",
        "print(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEMMATIZATION"
      ],
      "metadata": {
        "id": "ounJnUylRYJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Program: Lemmatization using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Input text\n",
        "text = \"Cats running faster than dogs after their food\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Create an object of WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each word\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Print the lemmatized words\n",
        "print(\"Original Words: \", tokens)\n",
        "print(\"Lemmatized Words: \", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcoKEMogQZ8a",
        "outputId": "3a44588a-62ab-4121-e710-828e64bb18a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words:  ['Cats', 'running', 'faster', 'than', 'dogs', 'after', 'their', 'food']\n",
            "Lemmatized Words:  ['Cats', 'running', 'faster', 'than', 'dog', 'after', 'their', 'food']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING"
      ],
      "metadata": {
        "id": "9E25Gr9htdOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "words = input(\"Enter words separated by space: \").split()\n",
        "\n",
        "\n",
        "for w in words:\n",
        "    print(f\"{w} → {stemmer.stem(w)}\")\n",
        "\n",
        "print(\"\\nHence, the program is successfully executed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7q19QVCQn9l",
        "outputId": "2449a825-6bdf-4f13-e1d0-03e64649c026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter words separated by space: hi hello\n",
            "hi → hi\n",
            "hello → hello\n",
            "\n",
            "Hence, the program is successfully executed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MORPHOLOGY**"
      ],
      "metadata": {
        "id": "VXxJAg64Rs9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Input text\n",
        "text = \"Morning runners easily ate\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform stemming\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Display results\n",
        "print(\"Original  :\", tokens)\n",
        "print(\"Stemmed   :\", stems)\n",
        "print(\"Lemmatized:\", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mnTLA7QRxcN",
        "outputId": "48d5994e-5126-4f3e-837a-970d30bf0515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original  : ['Morning', 'runners', 'easily', 'ate']\n",
            "Stemmed   : ['morn', 'runner', 'easili', 'ate']\n",
            "Lemmatized: ['Morning', 'runner', 'easily', 'ate']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPELLING **CORRECTION**"
      ],
      "metadata": {
        "id": "tHFF4brtR3Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Input a sentence from the user\n",
        "text = input(\"Enter a sentence: \")\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform spelling correction\n",
        "corrected = blob.correct()\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Sentence :\", text)\n",
        "print(\"Corrected Sentence:\", corrected)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR3b7t_ZR-H3",
        "outputId": "aa331143-2b2f-4ce4-aa2e-0efa2066cecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: happ isa haol\n",
            "Original Sentence : happ isa haol\n",
            "Corrected Sentence: happy isa hall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEDUCTION**"
      ],
      "metadata": {
        "id": "XrGUyU05SJeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.sem import Expression\n",
        "from nltk.inference import ResolutionProver\n",
        "\n",
        "read_expr = Expression.fromstring\n",
        "\n",
        "# Knowledge base (facts)\n",
        "kb = [\n",
        "    read_expr('man(Socrates)'),\n",
        "    read_expr('all x (man(x) -> mortal(x))')\n",
        "]\n",
        "\n",
        "# Hypothesis to prove\n",
        "goal = read_expr('mortal(Socrates)')\n",
        "\n",
        "# Try to prove the hypothesis using logical deduction\n",
        "print(ResolutionProver().prove(goal, kb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8MKE_FSSQhF",
        "outputId": "4782bf18-53bf-4139-8c76-578856b8a5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N GRAM MODELS"
      ],
      "metadata": {
        "id": "xy4gVDVZSXk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uni & Bigram\n",
        "def generate_ngrams(text, n):\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Create the n-grams using zip\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "\n",
        "    # Join the n-grams into a string and return\n",
        "    return [' '.join(gram) for gram in ngrams]\n",
        "\n",
        "# Example usage\n",
        "text = \"I love programming in Python\"\n",
        "\n",
        "# Generate unigrams (n = 1)\n",
        "unigrams = generate_ngrams(text, 1)\n",
        "\n",
        "# Generate bigrams (n = 2)\n",
        "bigrams = generate_ngrams(text, 2)\n",
        "trigarms = generate_ngrams(text, 3)\n",
        "\n",
        "# Print the results\n",
        "print(\"Unigrams:\")\n",
        "for unigram in unigrams:\n",
        "    print(unigram)\n",
        "\n",
        "print(\"\\nBigrams:\")\n",
        "for bigram in bigrams:\n",
        "    print(bigram)\n",
        "print(\"\\nTrigrams:\")\n",
        "for trigram in trigarms:\n",
        "    print(trigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5ysGgZ03g2U",
        "outputId": "46212cba-66be-42c4-fdb0-a1b90fb198ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:\n",
            "I\n",
            "love\n",
            "programming\n",
            "in\n",
            "Python\n",
            "\n",
            "Bigrams:\n",
            "I love\n",
            "love programming\n",
            "programming in\n",
            "in Python\n",
            "\n",
            "Trigrams:\n",
            "I love programming\n",
            "love programming in\n",
            "programming in Python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-GRAM **SMOOTHING**"
      ],
      "metadata": {
        "id": "8Kq_wCJYTKvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Program: Add-One Smoothing for Bigram Probability\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Input text\n",
        "text = \"I love NLP and I love Python\"\n",
        "\n",
        "# Step 1: Preprocess text\n",
        "words = text.lower().split()\n",
        "\n",
        "# Step 2: Create bigrams\n",
        "bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
        "\n",
        "# Step 3: Count unigrams and bigrams\n",
        "uni_count = Counter(words)\n",
        "bi_count = Counter(bigrams)\n",
        "\n",
        "# Step 4: Vocabulary size\n",
        "v = len(uni_count)\n",
        "\n",
        "# Step 5: Calculate Add-One (Laplace) Smoothed Bigram Probabilities\n",
        "print(\"Add-One Smoothed Bigram Probabilities:\\n\")\n",
        "for (w1, w2), count in bi_count.items():\n",
        "    prob = (count + 1) / (uni_count[w1] + v)\n",
        "    print(f\"P({w2}|{w1}) = {prob:.3f}\")\n",
        "\n",
        "print(\"\\nThus, the Python code on N-gram smoothing is successfully executed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmxyt2-NTTit",
        "outputId": "58779881-c540-4e7a-e698-035c1f6e65b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add-One Smoothed Bigram Probabilities:\n",
            "\n",
            "P(love|i) = 0.429\n",
            "P(nlp|love) = 0.286\n",
            "P(and|nlp) = 0.333\n",
            "P(i|and) = 0.333\n",
            "P(python|love) = 0.286\n",
            "\n",
            "Thus, the Python code on N-gram smoothing is successfully executed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HMM** **TAGGING**"
      ],
      "metadata": {
        "id": "el3FeSVdT35Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# Training Data\n",
        "train_data = [\n",
        "    [('The', 'DT'), ('dog', 'NN'), ('saw', 'VB'), ('a', 'DT'), ('cat', 'NN')],\n",
        "\n",
        "]\n",
        "\n",
        "# Train HMM Tagger\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train(train_data)\n",
        "print(\"--- HMM Tagger Trained ---\")\n",
        "\n",
        "# Test Sentences\n",
        "tests = [\n",
        "    ['The', 'cat', 'saw', 'the', 'dog'],\n",
        "\n",
        "]\n",
        "\n",
        "# Test and Print Results\n",
        "for s in tests:\n",
        "    print(f\"\\nInput: {s}\")\n",
        "    print(\"Tagged:\", tagger.tag(s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1SqJl83W0vN",
        "outputId": "c4dbd987-80f2-4cd4-ef7c-97d4f531ce9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- HMM Tagger Trained ---\n",
            "\n",
            "Input: ['The', 'cat', 'saw', 'the', 'dog']\n",
            "Tagged: [('The', 'DT'), ('cat', 'NN'), ('saw', 'VB'), ('the', 'DT'), ('dog', 'DT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS **TAGGING**"
      ],
      "metadata": {
        "id": "Tk0xSUxETbf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS TAGGING IN NLTK\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Input text\n",
        "text = \"No other power is more powerfull than the power primate.Chorooo!.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Display results\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"POS Tags:\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word:10} --> {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCfuu0miXeHe",
        "outputId": "09b3fb18-f99f-4acb-9fa3-94501291f3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['No', 'other', 'power', 'is', 'more', 'powerfull', 'than', 'the', 'power', 'primate.Chorooo', '!', '.']\n",
            "POS Tags:\n",
            "No         --> DT\n",
            "other      --> JJ\n",
            "power      --> NN\n",
            "is         --> VBZ\n",
            "more       --> RBR\n",
            "powerfull  --> JJ\n",
            "than       --> IN\n",
            "the        --> DT\n",
            "power      --> NN\n",
            "primate.Chorooo --> NN\n",
            "!          --> .\n",
            ".          --> .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BENDING POS TAGG"
      ],
      "metadata": {
        "id": "muojFRwTlKvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bending pos tagg\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"Python is a powerful programming language.\"\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8O6mjKYYBup",
        "outputId": "d106d9a9-97ee-4f94-f78b-2df4e0d619bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('programming', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    }
  ]
}